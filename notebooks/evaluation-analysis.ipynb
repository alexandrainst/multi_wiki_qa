{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyperclip\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path(\"data\", \"raw\", \"euroeval_benchmark_results.jsonl\")\n",
    "df = pd.read_json(results_path, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"language\"] = df.dataset_languages.map(lambda x: x[0] if len(x) == 1 else x)\n",
    "df[\"score\"] = df.results.map(lambda x: x[\"total\"][\"test_f1\"])\n",
    "df[\"standard_error\"] = df.results.map(lambda x: x[\"total\"][\"test_f1_se\"])\n",
    "df = df[[\"model\", \"language\", \"score\", \"standard_error\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by language and calculate the mean of the 'score' column.\n",
    "mean_scores_by_lang = df.groupby(\"language\")[\"score\"].mean()\n",
    "\n",
    "# Sort these mean scores in descending order (best performing languages first).\n",
    "mean_scores_by_lang_sorted = mean_scores_by_lang.sort_values(ascending=False)\n",
    "\n",
    "# Get the list of language names in this new sorted order.\n",
    "# This list will dictate the order of bars on the x-axis for all subplots.\n",
    "sorted_languages = mean_scores_by_lang_sorted.index.tolist()\n",
    "\n",
    "print(\"\\nLanguages sorted by mean performance:\")\n",
    "print(sorted_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Code ---\n",
    "\n",
    "models = df[\"model\"].unique()\n",
    "n_languages = len(sorted_languages)\n",
    "\n",
    "# --- Create the Subplot Grid ---\n",
    "nrows = 2\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 8), sharey=True)\n",
    "\n",
    "# --- Loop and Plot ---\n",
    "for ax, model in zip(axes.flat, models):\n",
    "    model_data = df[df[\"model\"] == model]\n",
    "    model_data_sorted = model_data.set_index(\"language\").loc[sorted_languages]\n",
    "    scores = model_data_sorted[\"score\"]\n",
    "\n",
    "    x_pos = np.arange(n_languages)\n",
    "    ax.bar(x_pos, scores, align=\"center\", alpha=0.8)\n",
    "\n",
    "    # --- Customize each subplot ---\n",
    "    ax.set_title(model, fontsize=14)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.tick_params(axis=\"x\", length=0)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    if ax.get_subplotspec().is_first_col():\n",
    "        ax.set_ylabel(\"F1-score\", fontsize=12)\n",
    "\n",
    "# --- Set Y-Axis Limits and Format for ALL Subplots ---\n",
    "plt.ylim(0, 100)\n",
    "formatter = mticker.PercentFormatter(xmax=100)\n",
    "axes[0, 0].yaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save the Figure ---\n",
    "\n",
    "# 1. Define the output path using pathlib.Path\n",
    "output_path = Path(\"data/final/evaluation-plot.png\")\n",
    "\n",
    "# 2. Create the parent directories if they don't exist\n",
    "# This prevents an error if 'data/final/' is not already created.\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3. Save the figure with high quality settings\n",
    "#    - dpi=300 is a good resolution for publications.\n",
    "#    - bbox_inches='tight' removes excess white space around the plot.\n",
    "fig.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# 4. (Optional) Also display the plot on screen\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved successfully to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"model\").score.apply(\n",
    "    lambda x: f\"{x.mean():.2f} ± {x.std(ddof=1) / np.sqrt(len(x)):.2f}\"\n",
    ").sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by=\"language\").set_index(\"language\")\n",
    "data_dict = dict()\n",
    "for model in sorted_df.model.unique():\n",
    "    model_df = sorted_df.query(\"model == @model\")\n",
    "    data_dict[\"Language\"] = model_df.index.tolist()\n",
    "    data_dict[model.split(\"/\")[-1]] = [\n",
    "        f\"{row.score:.1f}% ± {1.96 * row.standard_error:.1f}%\"\n",
    "        for _, row in model_df.iterrows()\n",
    "    ]\n",
    "score_df = pd.DataFrame(data_dict).reset_index()\n",
    "score_df = score_df.rename(\n",
    "    columns={\n",
    "        \"Mistral-Small-3.1-24B-Base-2503\": \"Mistral Base\",\n",
    "        \"Mistral-Small-3.1-24B-Instruct-2503\": \"Mistral Instruct\",\n",
    "        \"Llama-3.1-8B\": \"Llama Base\",\n",
    "        \"Llama-3.1-8B-Instruct\": \"Llama Instruct\",\n",
    "        \"xlm-roberta-large\": \"XLM-RoBERTa\",\n",
    "        \"multilingual-e5-large\": \"Multi-E5\",\n",
    "    }\n",
    ")\n",
    "score_df = score_df[\n",
    "    [\n",
    "        \"Language\",\n",
    "        \"Mistral Base\",\n",
    "        \"Mistral Instruct\",\n",
    "        \"Llama Base\",\n",
    "        \"Llama Instruct\",\n",
    "        \"XLM-RoBERTa\",\n",
    "        \"Multi-E5\",\n",
    "    ]\n",
    "]\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 65\n",
    "all_latex_tables = []\n",
    "for start in range(1, len(score_df), chunk_size):\n",
    "    score_df_chunk = score_df.iloc[start : start + chunk_size]\n",
    "    latex_table = (\n",
    "        score_df_chunk.to_latex(index=False)\n",
    "        .replace(\"%\", \"\\\\%\")\n",
    "        .replace(\"lllllll\", \"crrrrrr\")\n",
    "    )\n",
    "    latex_table = (\n",
    "        \"\\\\begin{table*}[h]\\n\\\\centering\\n\\\\scriptsize\\n\"\n",
    "        + latex_table\n",
    "        + \"\\\\caption{The F1-scores on MultiWikiQA for languages \"\n",
    "        + f\"{start}-{start + len(score_df_chunk)}\"\n",
    "        + \", sorted alphabetically.}\\n\"\n",
    "        + \"\\\\label{tab:num-samples-in-small-subsets}\\n\\\\end{table*}\"\n",
    "    )\n",
    "    all_latex_tables.append(latex_table)\n",
    "pyperclip.copy(\"\\n\\n\".join(all_latex_tables))\n",
    "print(\"All tables copied!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
