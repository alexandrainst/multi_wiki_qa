{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path(\"data\", \"raw\", \"euroeval_benchmark_results.jsonl\")\n",
    "df = pd.read_json(results_path, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"language\"] = df.dataset_languages.map(lambda x: x[0] if len(x) == 1 else x)\n",
    "df[\"score\"] = df.results.map(lambda x: x[\"total\"][\"test_f1\"])\n",
    "df[\"standard_error\"] = df.results.map(lambda x: x[\"total\"][\"test_f1_se\"])\n",
    "df = df[[\"model\", \"language\", \"score\", \"standard_error\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by language and calculate the mean of the 'score' column.\n",
    "mean_scores_by_lang = df.groupby(\"language\")[\"score\"].mean()\n",
    "\n",
    "# Sort these mean scores in descending order (best performing languages first).\n",
    "mean_scores_by_lang_sorted = mean_scores_by_lang.sort_values(ascending=False)\n",
    "\n",
    "# Get the list of language names in this new sorted order.\n",
    "# This list will dictate the order of bars on the x-axis for all subplots.\n",
    "sorted_languages = mean_scores_by_lang_sorted.index.tolist()\n",
    "\n",
    "print(\"\\nLanguages sorted by mean performance:\")\n",
    "print(sorted_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Code ---\n",
    "\n",
    "models = df[\"model\"].unique()\n",
    "n_languages = len(sorted_languages)\n",
    "\n",
    "# --- Create the Subplot Grid ---\n",
    "nrows = 2\n",
    "ncols = 3\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 8), sharey=True)\n",
    "\n",
    "# --- Loop and Plot ---\n",
    "for ax, model in zip(axes.flat, models):\n",
    "    model_data = df[df[\"model\"] == model]\n",
    "    model_data_sorted = model_data.set_index(\"language\").loc[sorted_languages]\n",
    "    scores = model_data_sorted[\"score\"]\n",
    "\n",
    "    x_pos = np.arange(n_languages)\n",
    "    ax.bar(x_pos, scores, align=\"center\", alpha=0.8)\n",
    "\n",
    "    # --- Customize each subplot ---\n",
    "    ax.set_title(model, fontsize=14)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.tick_params(axis=\"x\", length=0)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    if ax.get_subplotspec().is_first_col():\n",
    "        ax.set_ylabel(\"F1-score\", fontsize=12)\n",
    "\n",
    "# --- Set Y-Axis Limits and Format for ALL Subplots ---\n",
    "plt.ylim(0, 100)\n",
    "formatter = mticker.PercentFormatter(xmax=100)\n",
    "axes[0, 0].yaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Add an overarching title\n",
    "fig.suptitle(\"Model Performance (Languages Sorted by Mean Score)\", fontsize=18, y=1.02)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save the Figure ---\n",
    "\n",
    "# 1. Define the output path using pathlib.Path\n",
    "output_path = Path(\"data/final/evaluation-plot.png\")\n",
    "\n",
    "# 2. Create the parent directories if they don't exist\n",
    "# This prevents an error if 'data/final/' is not already created.\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3. Save the figure with high quality settings\n",
    "#    - dpi=300 is a good resolution for publications.\n",
    "#    - bbox_inches='tight' removes excess white space around the plot.\n",
    "fig.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# 4. (Optional) Also display the plot on screen\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved successfully to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"model\").score.apply(\n",
    "    lambda x: f\"{x.mean():.2f} Â± {x.std(ddof=1) / np.sqrt(len(x)):.2f}\"\n",
    ").sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in df.model.unique():\n",
    "    model_df = df.query(\"model == @model\").drop(columns=[\"model\", \"standard_error\"])\n",
    "    model_df.score = model_df.score.map(lambda x: f\"{x:.0f}\")\n",
    "    model_df.rename(columns=dict(score=\"F1-score\"), inplace=True)\n",
    "    print(f\"=== Table for {model} ===\")\n",
    "    print(model_df.to_latex(index=False), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
